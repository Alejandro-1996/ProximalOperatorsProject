{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "single-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technical libraries\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import scipy \n",
    "import scipy.fftpack\n",
    "import proximal \n",
    "from iplabs import IPLabViewer as viewer\n",
    "import matplotlib\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-merchandise",
   "metadata": {},
   "source": [
    "# Proximal Operators for Nonnegative Inverse Problems\n",
    "\n",
    "Study of the combination of different image regularizers with nonnegativity constraints.\n",
    "\n",
    "# Index\n",
    "\n",
    "1. [Introductionr](#Intro)\n",
    "    1. [Proximal Operator](#Prox)\n",
    "    2. [Nonnegativity Function](#Nonneg)\n",
    "2. [$\\operatorname{l}_1$](#L1)\n",
    "3. [$\\mathrm{L^2}$](#L2)\n",
    "4. [Matrix Q](#MatQ)\n",
    "5. [Evaluating](#Eval)\n",
    "\n",
    "# <a name=\"Intro\"></a>1. Introduction\n",
    "\n",
    "The goal of the present notebook is to find how of common image regularizers combine with nonnegativity through the proximal operator. We will be studying regularizers likethe $\\mathrm{L}^1$ and the $\\mathrm{L}^2$ norm, and group sparsity.\n",
    "\n",
    "## <a name=\"Prox\"></a>1.A. Proximal Operator\n",
    "\n",
    "The proximal operator of a function $f$ is:\n",
    "\n",
    "$$\\mathrm{prox}_f(v) = \\arg \\min_x(f(x)+\\frac{1}{2\\lambda}||x - v||_2^2)$$\n",
    "\n",
    "This means that it will optimize an input vector $v$ with respect to a function, but adding the constraint that the result has to be *somewhat close* (by the minimization of the second term, and with *somewhat* parametrized by $\\lambda$) to the original. \n",
    "\n",
    "The interest of the project is to see how common image regularizers combine with nonnegativity constraints. In particular, we will see wether:\n",
    "$$\\mathrm{prox}_{f} = \\mathrm{prox}_{\\delta_{\\rm I\\!R_+^N}}(\\mathrm{prox}_{\\Re})$$\n",
    "or \n",
    "$$\\mathrm{prox}_{f} = \\mathrm{prox}_{\\Re}(\\mathrm{prox}_{\\delta_{\\rm I\\!R_+^N}})$$\n",
    "where $f = \\delta_{\\rm I\\!R_+^N} + \\Re$,\n",
    "for several regularizers, starting by the know cases of $\\mathrm{L^1}$ and $\\mathrm{L^2}$ norms. \n",
    "\n",
    "In some cases for which the $\\mathrm{prox}$ is a point-wise operation, a plot of $v_i$ vs $x_i$ can be made, where $v_i$ is the $i^{th}$ element of $v$. In this cases, the previous equations can be provedor discarded analitically.  \n",
    "\n",
    "## <a name=\"Nonneg\"></a>1.B. Nonnegativity Function\n",
    "\n",
    "The nonnegativity function $\\delta$ is defined as:\n",
    "\n",
    "$$\\delta_{\\rm I\\!R_+^N} =\n",
    "\\begin{cases}\n",
    "        0 \\mathrm{ if } x \\in \\rm I\\!R_+^N \\\\\n",
    "        + \\inf \\mathrm{ if } x\\notin \\rm I\\!R_+^N\n",
    "     \\end{cases}$$\n",
    "     \n",
    "Given the definition of the proximal operator, it is immediate to see that to solve the proximal operator, we have as constraint $x\\in \\rm I\\!R_+^N$. As such, the first term vanishes (since $x\\in \\rm I\\!R_+^N$, by definition $f(x) = 0$). As such, we arrive to:\n",
    "\n",
    "$$prox_{\\rm I\\!R_+^N}(v) = \\arg \\min_{x\\in \\rm I\\!R_+^N}\\left(\\frac{1}{2}||x - v||_2^2\\right)$$\n",
    "\n",
    "Given that $v \\in \\rm I\\!R_+^N$, then the $x\\in \\rm I\\!R_+^N$ that minimizes the prox operator is simply the $x$ closest to $v$, but yet inside the domain imposed by the indicator function (indicator of nonnegativity). It is useless to include the parameter $\\lambda$ in this proximal, as the minimization of squares is the only term present.\n",
    "\n",
    "In the next cell we define the nonnegativity proximal operator, and then make a plot of $v_i$ vs $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beginning-turkey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc71f4db40449cab23e85632b5a096a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Pan', 'Pan axes with left…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x122941ac288>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonneg = lambda x: np.maximum(np.zeros_like(x), x)\n",
    "\n",
    "vi = np.arange(-100, 100)\n",
    "plt.figure()\n",
    "plt.title('Proximal of Nonnegativity Constraint')\n",
    "plt.grid()\n",
    "plt.xlabel('v')\n",
    "plt.ylabel('x')\n",
    "plt.plot(vi, list(map(nonneg, vi)), label = 'Prox\\u03b4')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-advancement",
   "metadata": {},
   "source": [
    "# <a name = 'L1'></a> 2. L1 Norm\n",
    "[Back to Index](#Index)\n",
    "\n",
    "In this section we will prove the equations provided on [1.A](#Prox), for the $\\mathrm{L}^1$ norm. It is a part from the family of the $L^p$ norms where $p = 1$, is defined for a vector $x$ as:\n",
    "$$\\|\\mathbf{x}\\|_p = (\\sum_{i = 1}^n \\mathbf{x}^p)^\\frac{1}{p} $$\n",
    "\n",
    "It follows that the $L^1$ norm of a vector is simply the sum of its components. It is a common image regularizer because it enforces sparsity, a well known property of natural images. \n",
    "\n",
    "### <a name = 'L1_Prox'></a> 2.A Proximal \n",
    "[Back to Index](#Index)\n",
    "\n",
    "Following the definition of the Proximal operator:\n",
    "\n",
    "$$\\mathrm{prox}_{L^1}(\\mathbf{v}) = \\arg \\min_{\\mathbf{x}}(\\|\\mathbf{x}\\|_1 + \\frac{1}{2\\lambda}||\\mathbf{x} - \\mathbf{v}||_2^2))$$\n",
    "\n",
    "An intuitive interpretation is that the solution $x$ will be one where each element $x_i$ of $x$ is pulled towards $0$ in order to minimize  the $L^1$ norm (clearly without crossing $0$), but still remaining close to $v$ (to reduce the squared term). How much is it pulled depends on the parameter $\\lambda$.\n",
    "\n",
    "It follows that the $\\mathrm{prox}$ is indeed a point-wise operation, defined as:\n",
    "\\mathrm{}\n",
    "$$\\mathrm{prox}_{\\lambda \\mathrm{L}^1}(x_i) = \\mathrm{sign}(x_i)\\mathrm{max}(|x_i| - \\lambda, 0)$$\n",
    "Run the next cell to show this effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tested-kidney",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029abfc48e8741c89350b4e6bff327a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Pan', 'Pan axes with left…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1229716f148>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_prox = lambda x, lamb: np.sign(x)*np.maximum(np.abs(x) - lamb, 0)\n",
    "\n",
    "lamb = 10\n",
    "vi = np.arange(-100, 100)\n",
    "plt.figure()\n",
    "plt.title('Proximal of $\\|\\| \\cdot \\|\\|_1$ Norm')\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(r'$\\mathrm{prox}_{\\lambda \\|\\| \\cdot \\|\\|_1}$(x)')\n",
    "plt.axvline(lamb, color='r', linewidth=0.5, label='\\u03bb')\n",
    "plt.axvline(-lamb, color='r', linewidth=0.5, label='-\\u03bb')\n",
    "plt.plot(vi, list(map(l1_prox, vi, np.ones_like(vi)*lamb)), label = r'$\\mathrm{prox}_{\\lambda \\|\\| \\cdot \\|\\|_1}$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-producer",
   "metadata": {},
   "source": [
    "### <a name = 'L1_Prox+Nonneg'></a> 2.B $\\mathrm{L}^1$ + Nonneg\n",
    "[Back to Index](#Index)\n",
    "\n",
    "In this section we will compute the proximal operator of the $\\mathrm{L}^1$ norm plus the nonnegativity constraint:\n",
    "$$\\mathrm{prox}_f(v)$$\n",
    "where   \n",
    "$$f(x) = ||x||_1 +  \\delta_{\\rm I\\!R_+^N}$$\n",
    "\n",
    "Following the definition from [section 1.B](#Proximal), and the derivation of the proximal from the nonnegativity constraint, it follows that:\n",
    "\n",
    "$$\\mathrm{prox}_{f}(v) = \\arg \\min_{x\\in \\rm I\\!R_+^N}(||x||_1 + \\frac{1}{2\\lambda}||x - v||_2^2))$$\n",
    "\n",
    "In this term we could add the function $\\delta_{\\rm_+^N}$, but since we have already defined $x\\in \\rm I\\!R_+^N$, its value vanishes. \n",
    "\n",
    "From the constraint in the domain of $x$ and the structure of the $\\mathrm{prox}_f(v)$, it is clear that the analytical solution with be similar to the solution of $\\mathrm{prox}_{\\mathrm{L^1}}(x)$, buth with the constraint in the domain applied:\n",
    "\n",
    "$$\\mathrm{prox}_{\\lambda \\mathrm{L}^1 + \\delta_{\\rm_+^N}}(v_i) = \\mathrm{max}(v_i - \\lambda, 0)$$\n",
    "\n",
    "Run the next cell to define this function and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "identical-lancaster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a889f9f34649a7928df65bcd5d2012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Pan', 'Pan axes with left…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x122972a5c88>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_plus_nonneg_prox = lambda x, lamb: np.maximum(x - lamb, 0)\n",
    "\n",
    "lamb = 10\n",
    "\n",
    "vi = np.arange(-100, 100)\n",
    "plt.figure()\n",
    "plt.title('Proximal of L1 Norm')\n",
    "plt.grid()\n",
    "plt.xlabel('v')\n",
    "plt.ylabel('x')\n",
    "plt.axvline(lamb, color='r', linewidth=0.5, label='\\u03bb')\n",
    "plt.plot(vi, list(map(l1_plus_nonneg_prox, vi, np.ones_like(vi)*lamb)), label='Prox L1+\\u03b4')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-welding",
   "metadata": {},
   "source": [
    "### <a name = 'Prox_L1(Prox(Nonneg))'></a> 2.C $\\mathrm{prox}_{\\mathrm{L}^1}(\\mathrm{prox}_{\\delta_{\\rm I\\!R_+^N }}(x))$\n",
    "[Back to Index](#Index)\n",
    "\n",
    "In this section we will:\n",
    " * Compute $\\mathrm{prox}_{\\mathrm{L}^1}(\\mathrm{prox}_{\\delta_{\\rm I\\!R_+^N}}(x))$\n",
    " * Verify wether it is equal to $\\mathrm{prox}_{\\mathrm{L}^1\\delta_{\\rm I\\!R_+^N}}(x))$\n",
    " \n",
    "This is simply a composition of the two functions that we have already defined, `l1_prox` and `nonneg`. \n",
    "\n",
    "Run the next to apply both operators and plot them. Moreover, on the same plot we will plot $\\mathrm{prox}_{\\mathrm{L}^1\\delta_{\\rm_+}}(x))$, and verify wether it is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "partial-alabama",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80724dccabe4c1a9ab4b85e8a6f08c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Pan', 'Pan axes with left…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prox_l1(prox_nonneg(v)) IS equal to prox(l1 + nonneg)\n"
     ]
    }
   ],
   "source": [
    "vi = np.arange(-100, 100)\n",
    "nonneg_vect = list(map(nonneg, vi))\n",
    "final_vect = list(map(l1_prox, nonneg_vect, np.ones_like(vi)*lamb))\n",
    "l1_plus_nonneg_vect = list(map(l1_plus_nonneg_prox, vi, np.ones_like(vi)*lamb))\n",
    "plt.figure()\n",
    "plt.title('Proximal of L1 Norm')\n",
    "plt.grid()\n",
    "plt.xlabel('v')\n",
    "plt.ylabel('x')\n",
    "plt.axvline(lamb, color='r', linewidth=0.5, label='\\u03bb')\n",
    "plt.plot(vi, final_vect, ':', linewidth=3, label=r'$\\mathrm{prox}_{L1}(Prox_\\delta)(x)$')\n",
    "plt.plot(vi, l1_plus_nonneg_vect, '--', label='Prox_L1+\\u03b4(x)')\n",
    "plt.legend()\n",
    "\n",
    "if np.array_equal(final_vect, l1_plus_nonneg_vect):\n",
    "    print(f'prox_l1(prox_nonneg(v)) IS equal to prox(l1 + nonneg)')\n",
    "else:\n",
    "    print(f'prox_l1(prox_nonneg(v)) IS NOT equal to prox(l1 + nonneg)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-paradise",
   "metadata": {},
   "source": [
    "### <a name = 'Prox_L1(Prox(Nonneg))'></a> 2.D $\\mathrm{prox}_{\\mathrm{L}^1}(\\mathrm{prox}_{\\delta_{\\rm I\\!R_+^N }}(x))$\n",
    "[Back to Index](#Index)\n",
    "\n",
    "In this section we will do the same as in last section but in the inverse order ($\\mathrm{prox}_{\\delta_{\\rm I\\!R_+^N}(\\mathrm{prox}_{\\mathrm{L}^1}}(x))$ instead of $\\mathrm{prox}_{\\mathrm{L}^1}(\\mathrm{prox}_{\\delta_{\\rm I\\!R_+^N}}(x))$)\n",
    "\n",
    "Run the next cell to see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "younger-interim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1529d0104734b3e86bddfa2eb2b422b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Pan', 'Pan axes with left…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prox_nonneg(prox_l1(v)) IS equal to prox(l1 + nonneg)\n"
     ]
    }
   ],
   "source": [
    "lamb = 10\n",
    "\n",
    "vi = np.arange(-100, 100)\n",
    "nonneg_vect = list(map(l1_prox, vi, np.ones_like(vi)*lamb))\n",
    "final_vect = list(map(nonneg, nonneg_vect))\n",
    "l1_plus_nonneg_vect = list(map(l1_plus_nonneg_prox, vi, np.ones_like(vi)*lamb))\n",
    "plt.figure()\n",
    "plt.title('Proximal of L1 Norm')\n",
    "plt.grid()\n",
    "plt.xlabel('v')\n",
    "plt.ylabel('x')\n",
    "plt.axvline(lamb, color='r', linewidth=0.5, label='\\u03bb')\n",
    "plt.plot(vi, final_vect, ':', linewidth=3, label='Prox_\\u03b4(Prox_L1)(x)')\n",
    "plt.plot(vi, l1_plus_nonneg_vect, '-.', label='Prox_L1+\\u03b4(x)')\n",
    "plt.legend()\n",
    "\n",
    "if np.array_equal(final_vect, l1_plus_nonneg_vect):\n",
    "    print(f'prox_nonneg(prox_l1(v)) IS equal to prox(l1 + nonneg)')\n",
    "else:\n",
    "    print(f'prox_nonneg(prox_l1(v)) IS NOT equal to prox(l1 + nonneg)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-gabriel",
   "metadata": {},
   "source": [
    "### 2.D Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-radius",
   "metadata": {},
   "source": [
    "$\\ell_2$ norm, $\\lambda$ = 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "spectacular-camcorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prox_nonneg(prox_reg(v)) SEEMS equal to prox(reg + nonneg).\n",
      "Max absolute error: 0.000e+00\n",
      "Average absolute error: 0.000e+00\n",
      "\n",
      "prox_reg(prox_nonneg(v)) SEEMS equal to prox(reg + nonneg)\n",
      "Max absolute error: 0.000e+00\n",
      "Average absolute error: 0.000e+00\n",
      "\n",
      "Plotting example\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994bfe77c03445a0b32aef77f70f6bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Pan', 'Pan axes with left…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '$\\\\mathrm{prox}_{\\\\ell_1 + \\\\delta}(\\\\mathbf{v})$')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print('L1 Norm:')\n",
    "proximal.evaluate(l1_prox, l1_plus_nonneg_prox, n=100, size=(30,), mean=0, sigma=1, lamb=0.5, plot=True,)\n",
    "plt.title('$\\mathrm{prox}_{\\ell_1 + \\delta}(\\mathbf{v})$', fontsize = 17)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-roberts",
   "metadata": {},
   "source": [
    "# <a name = 'L2'></a> 3. L2 Norm\n",
    "[Back to Index](#Index)\n",
    "\n",
    "In this section we will prove the equations provided on [1.A](#Prox), for the $\\mathrm{L}^2$ norm, in a similar way as we did in section [2](#L1). \n",
    "\n",
    "### <a name = 'L2_Prox'></a> 3.A Proximal \n",
    "[Back to Index](#Index)\n",
    "\n",
    "The proximal operator of the L2 Norm is solved by projection onto the L2 unit ball $\\Pi_B$, which follows from Moreau decomposition:\n",
    "\n",
    "$$\\mathrm{prox}_{L^2}(v) = v - \\lambda \\Pi_B\\frac{v}{\\lambda}$$\n",
    "\n",
    "Thus, the projection $\\Pi_B$ is simple. A vector $v$ remains unchanged if it is already inside $\\Pi_B$, and it can be projected into the ball by normalizaation by its own norm otherwise. It is clear from the above that if the point $\\frac{v}{\\lambda}$ is within $\\Pi_B$, the value of the $\\mathrm{prox}_{L^2}(v)$ will vanish. Thus:\n",
    "\n",
    "$$\\mathrm{prox}_{L^2}(v) =\n",
    "\\begin{cases}\n",
    "        (1 - \\frac{\\lambda}{||v||_2})v if ||v||_2 \\geq \\lambda \\\\\n",
    "        0 if ||v||_2 < \\lambda\n",
    "     \\end{cases}$$\n",
    "\n",
    "It is clear that the only interesting scenario is when $||v||_2 \\geq \\lambda$. It follows that when $||v||_2$ is fixed, the $\\mathrm{prox}$ is indeed a point-wise operation, defined as:\n",
    "$$\\mathrm{prox}_{\\lambda \\mathrm{L}^2,}(x_i) = (1 - \\lambda v)$$\n",
    "$$\\mathrm{s.t.} \\lambda < 1, ||v||_2 = 1 $$\n",
    "\n",
    "However, if $||v||_2 \\neq 1$ it is easy to generalize the $\\mathrm{prox}$ of the L2 norm as a scaling a factor \n",
    "\n",
    "$$\\mathrm{prox}_{L^2}(v_i) = \\left(1 - \\frac{\\lambda}{\\min \\left( \\|\\mathbf{v}\\|_2, \\lambda \\right)}\\right) v_i$$\n",
    "\n",
    "Run the next cell to show $v$ vs $x$ for a parametrized version of the ($||v||_2 = 1$). Additionally, we will include the non-parametrized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "british-congress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac2b738b4e04b4e950d79fa3d4fe1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Pan', 'Pan axes with left…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12297426cc8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_prox_param = lambda x, lamb: (1 - np.minimum(lamb, np.ones_like(x)))*x\n",
    "l2_prox = lambda x, lamb: (1 - np.minimum(lamb/np.linalg.norm(x, 2), np.ones_like(x)))*x\n",
    "lamb = 0.5\n",
    "\n",
    "vi = np.arange(-10, 11)\n",
    "plt.figure()\n",
    "plt.title('Proximal of $\\| \\cdot \\|\\|_2$ Norm')\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(r'$\\mathrm{prox}_{\\lambda \\|\\| \\cdot \\|\\|_2}$(x)')\n",
    "plt.ylim([-10, 10])\n",
    "plt.xlim([-10, 10])\n",
    "plt.plot(vi, list(map(l2_prox_param, vi, np.ones_like(vi)*lamb)), label = 'Prox L1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-amazon",
   "metadata": {},
   "source": [
    "### <a name = 'L2_Prox+Nonneg'></a> 3.B $\\mathrm{L}^2$ + Nonneg\n",
    "[Back to Index](#Index)\n",
    "\n",
    "As we have already seen, the $\\mathrm{prox}$ of $\\delta_{\\rm IR_+^N}$ imposes a constraint in the domain of the input vector $v$. Thus in this section we will compute the proximal operator of the $\\mathrm{L}^1$ norm plus the nonnegativity constraint$^{[1]}$:\n",
    "\n",
    "$$\\mathrm{prox}_{L^2 + \\delta_{\\rm I\\!R_+^N }}(v) = \\left(1 - \\frac{\\lambda}{\\mathrm{max}\\left(||\\mathrm{max}(x, 0)||_2, \\lambda\\right)} \\right)\\mathrm{max}(x, 0)$$\n",
    "\n",
    "It is explicit from the formula that $\\mathrm{prox}_{L^2 + \\delta_{\\rm I\\!R_+^N}}$ is acting on the projection into $\\rm I\\!R_+^N$. And thus, \n",
    "\n",
    "$$\\mathrm{prox}_{L^2 + \\delta_{\\rm I\\!R_+^N }} \\neq \\mathrm{prox}_{\\mathrm{L}^2}(\\mathrm{prox}_{\\delta_{\\rm I\\!R_+^N}})$$\n",
    "whereas in general (except for very specific cases) \n",
    "$$\\mathrm{prox}_{L^2 + \\delta_{\\rm I\\!R_+^N }} = \\mathrm{prox}_{\\delta_{\\rm I\\!R_+^N}}(\\mathrm{prox}_{\\Re})$$\n",
    "\n",
    "This case is clearly not a point wise operation independent of the input vector, and cannot be parametrized easily without loss of generalization (for we can assume a set of normalized vectors, but the norm will certainly change after applying nonnegativity constraints). Anyway, run the next cell to declare the correspinding map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "choice-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_plus_nonneg_prox = lambda x, lamb: (1 - lamb/(max(np.linalg.norm(np.maximum(x, 0), 2), np.max(lamb))))*np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-accuracy",
   "metadata": {},
   "source": [
    "### 3.C Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "alternative-nebraska",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Norm:\n",
      "prox_nonneg(prox_reg(v)) IS NOT equal to prox(reg + nonneg)\n",
      "Max absolute error: 2.124e-01\n",
      "Average absolute error: 1.641e-02\n",
      "\n",
      "prox_reg(prox_nonneg(v)) SEEMS equal to prox(reg + nonneg)\n",
      "Max absolute error: 0.000e+00\n",
      "Average absolute error: 0.000e+00\n",
      "\n",
      "Plotting example\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9719840f660b4b4dac165d89440dbbec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Pan', 'Pan axes with left…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '$\\\\mathrm{prox}_{\\\\lambda(\\\\ell_2 + \\\\delta})(\\\\mathbf{v})$')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('L2 Norm:')\n",
    "proximal.evaluate(l2_prox, l2_plus_nonneg_prox, n=100, size=(30,), mean=0, sigma=1, lamb=0.5, plot=True,)\n",
    "plt.title('$\\mathrm{prox}_{\\lambda(\\ell_2 + \\delta})(\\mathbf{v})$', fontsize = 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-retirement",
   "metadata": {},
   "source": [
    "### Minimization of L inf norm\n",
    "\n",
    "$\\ell_\\inf$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "included-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linf_prox(v, lamb):\n",
    "    \n",
    "    if len(v.shape) == 1:\n",
    "        nx = len(v)\n",
    "        x = cp.Variable((nx))\n",
    "    elif len(v.shape) == 2:\n",
    "        nx, ny = v.shape\n",
    "        x = cp.Variable((nx, ny))\n",
    "    \n",
    "    # Defining D1 and D2x We loose one element in the *other* dimension to account for the lost element and have elements of equal size\n",
    "    obj = cp.norm(x, 'inf') + cp.sum_squares(x - v)/(2*lamb)\n",
    "    \n",
    "    prob = cp.Problem(cp.Minimize(obj))\n",
    "#     prob.solve(solver=cp.ECOS)\n",
    "    prob.solve()\n",
    "    \n",
    "    return x.value\n",
    "\n",
    "def Linf_pust_nonneg_prox(v, lamb):\n",
    "        \n",
    "    if len(v.shape) == 1:\n",
    "        nx = len(v)\n",
    "        x = cp.Variable((nx),nonneg = True)\n",
    "    elif len(v.shape) == 2:\n",
    "        nx, ny = v.shape\n",
    "        x = cp.Variable((nx, ny),nonneg = True)\n",
    "        \n",
    "    # Defining D1 and D2x We loose one element in the *other* dimension to account for the lost element and have elements of equal size\n",
    "    obj = cp.norm(x, 'inf') + cp.sum_squares(x - v)/(2*lamb)\n",
    "    \n",
    "    prob = cp.Problem(cp.Minimize(obj))\n",
    "#     prob.solve(solver=cp.ECOS)\n",
    "    prob.solve()\n",
    "    \n",
    "    return x.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-display",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "danish-grill",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "||x||p:\n",
      "prox_nonneg(prox_reg(v)) IS NOT equal to prox(reg + nonneg)\n",
      "Max absolute error: 6.117e-01\n",
      "Average absolute error: 1.121e-01\n",
      "\n",
      "prox_reg(prox_nonneg(v)) SEEMS equal to prox(reg + nonneg)\n",
      "Max absolute error: 2.776e-23\n",
      "Average absolute error: 3.274e-25\n",
      "\n",
      "Plotting example\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17449becd42045a3a7289783cb77b265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Pan', 'Pan axes with left…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '$\\\\mathrm{prox}_{\\\\lambda (\\\\|DCT\\\\cdot \\\\|_1 + \\\\delta)}(\\\\mathbf{v})$')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\n||x||p:')\n",
    "proximal.evaluate(Linf_prox, Linf_pust_nonneg_prox, n=10, size=(40, ), mean=0, sigma=1, lamb=10, plot = True, err_thr = 1e-3)\n",
    "plt.title('$\\mathrm{prox}_{\\lambda (\\|DCT\\cdot \\|_1 + \\delta)}(\\mathbf{v})$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-liberal",
   "metadata": {},
   "source": [
    "## <a name = 'DCT'></a> 4. Minimization of $DCTx$\n",
    "\n",
    "In this case we will try to minimiza an $f(x) = g(Qx)$. To begin with a simple case, we will choose $Q$ to be the [Discrete Cosine Transform](https://docs.scipy.org/doc/scipy/reference/generated/scipy.fftpack.dct.html), so $f(x) = \\|DCTx\\|_1, \\lambda = 0.1$ which we know to be both orthogonal ($Q^TQ = QQ^T = I$) and real. Orthogonality has the advantage that it can be solved using the following property:\n",
    "$$\\mathrm{prox}_{\\lambda f}(v) =  Q^T\\mathrm{prox}_{\\lambda g}(Qv)$$\n",
    "where, in our case\n",
    "$$f(x) = ||Qx||_1$$\n",
    "\n",
    "### <a name = 'DCT_Prox'></a> 4.A Proximal\n",
    "\n",
    "If we choose $f(x) = \\|x\\|_1$, then by [section 2.A](#L1_Prox), we have:\n",
    "$$\\mathrm{prox}_{\\lambda f(x) = \\|Qx\\|_1}(v) = Q^T\\mathrm{sign}(Qv)\\odot \\mathrm{max}(|Qv| - \\lambda, 0)$$\n",
    "\n",
    "If we fix the size of $\\mathbf{v}$, this is evidently a pointwise operation. Run the next cell to declare $\\mathrm{prox}_{\\lambda f(x) = \\|Qx\\|_1}$ And plot the function as a point-wise operation (change the parameter $n$ to fix the isze of $\\mathbf{v}$).  \n",
    "<!-- $$f(x) = ||x||_1 +  \\delta_{\\rm I\\!R_+^N}$$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adjustable-progress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a940026d51413e9c62c6b0bfbd9c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Pan', 'Pan axes with left…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x122974df288>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare prox as a function of x (vector), Q (Matrix), lamb (parameter)\n",
    "# l1_DCT_prox = lambda x, Q, lamb: Q.T @ np.sign(Q@x) * np.maximum(np.abs(Q@x) - lamb, 0)\n",
    "def DCT_l1_prox(v, lamb): \n",
    "    Q = scipy.fftpack.dct(np.eye(len(v)), norm = 'ortho')\n",
    "    return Q.T @ np.sign(Q@v) * np.maximum(np.abs(Q@v) - lamb, 0)\n",
    "\n",
    "n = 64\n",
    "v = np.arange(-n/2, n/2)\n",
    "Q = scipy.fftpack.dct(np.eye(n), norm = 'ortho')\n",
    "lamb = 5\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Proximal of $f(x) = \\|Qx\\|_1$ Norm')\n",
    "plt.grid()\n",
    "plt.xlabel('v')\n",
    "plt.ylabel('x')\n",
    "plt.plot(v, DCT_l1_prox(v, lamb), label = '$\\mathrm{Prox}_{\\lambda \\|Qv\\|_1}$')\n",
    "plt.plot(v, Q@v, label = 'DCT ($Qx$)')\n",
    "plt.ylim([-100, 100])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-identity",
   "metadata": {},
   "source": [
    "### <a name = 'MatQ_Prox+Nonneg'></a> 4.B $\\|Qx\\|_1$ + Nonneg\n",
    "[Back to Index](#Index)\n",
    "\n",
    "$$f(x) = ||Qx||_1 +  \\delta_{\\rm I\\!R_+^N}(x)$$\n",
    "\n",
    "From the definition of the proximal operator:\n",
    "\n",
    "$$\\mathrm{prox}_f(v) = \\arg \\min_{x \\in \\rm I\\!R_+^N}(\\|Qx\\|_1+\\frac{1}{2}||x - v||_2^2)$$\n",
    "\n",
    "The function $f(x)$ is no longer simply a composition function $f\\circ g$ (the nonnegativity constraint acts on $x$, not on $Qx$). As such, it is not trivial to find a closed form solution, and we have to declare a function minimizing it as an optimization problem, solved by CVXPy. Run the following cell to declare this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "opponent-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCT_l1_plus_nonneg(v, lamb):\n",
    "    \n",
    "    # Get dimension of v\n",
    "    n = len(v)\n",
    "    \n",
    "    # define DCT matrix\n",
    "    DCT = scipy.fftpack.dct(np.eye(n), norm = 'ortho')\n",
    "    # Define nonnegative constraint\n",
    "    x = cp.Variable(v.shape, nonneg = True) \n",
    "    # Define de the cost function\n",
    "    obj = cp.norm(DCT@x, 1) + cp.sum_squares(x - v)/(2*lamb)\n",
    "#     obj = cp.norm(scipy.fftpack.dct(x), 1) + cp.sum_squares(x - v)/(2*lamb)\n",
    "#     DCT@x == scipy.fftpack.dct(x)\n",
    "    prob = cp.Problem(cp.Minimize(obj))\n",
    "    prob.solve()\n",
    "    \n",
    "    return x.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-tunisia",
   "metadata": {},
   "source": [
    "### 4.C Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "attempted-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "||DCTx||1:\n",
      "prox_nonneg(prox_reg(v)) IS NOT equal to prox(reg + nonneg)\n",
      "Max absolute error: 1.637e+00\n",
      "Average absolute error: 2.921e-01\n",
      "\n",
      "prox_reg(prox_nonneg(v)) IS NOT equal to prox(reg + nonneg)\n",
      "Max absolute error: 2.744e+00\n",
      "Average absolute error: 4.277e-01\n",
      "\n",
      "Plotting example\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba5a53f0a5a452fa6b08490607ca8f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Pan', 'Pan axes with left…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '$\\\\mathrm{prox}_{\\\\lambda (\\\\|DCT\\\\cdot \\\\|_1 + \\\\delta)}(\\\\mathbf{v})$')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\n||DCTx||1:')\n",
    "proximal.evaluate(DCT_l1_prox, DCT_l1_plus_nonneg, n=1, size=(30,), mean=0, sigma=1, lamb=0.1, plot = True)\n",
    "plt.title('$\\mathrm{prox}_{\\lambda (\\|DCT\\cdot \\|_1 + \\delta)}(\\mathbf{v})$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-circuit",
   "metadata": {},
   "source": [
    "## <a name = 'MatQ'></a> 5. Minimization of $\\|Qx\\|_1$\n",
    "\n",
    "In this section we will try to minimize an $f(x) = \\|Qx\\|_1$, $\\lambda = 0.3$ , but this time the general case, without making any assumptions on $Q$. \n",
    "\n",
    "### <a name = 'MatQ_Prox'></a> 5.A Proximal\n",
    "\n",
    "By the definition of the proximal:\n",
    "$$\\mathrm{prox}_f(v) = \\arg \\min_{x}(\\|Qx\\|_1+\\frac{1}{2}||x - v||_2^2)$$\n",
    "\n",
    "Run the next cell to declare `Q_L1_prox`. In it, we will hard-code `Q`to be the finite differences matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "worthy-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_L1_prox_1D(v, lamb):\n",
    "    \n",
    "    if len(v.shape) > 1:\n",
    "        print('WARNING:\\nThis function is designed for 1D signals. Stopping')\n",
    "        return\n",
    "    # Get dimension of v\n",
    "    n = len(v)\n",
    "    \n",
    "    vector = np.zeros((n,))\n",
    "    vector[0] = 1\n",
    "    vector[1] = -1\n",
    "    idx = np.arange(n)\n",
    "    Q = np.matlib.repmat(vector, n, 1)\n",
    "    for i, row in enumerate(Q):\n",
    "        Q[i] = np.roll(row, i)\n",
    "    \n",
    "    # Define variable\n",
    "    x = cp.Variable(n) \n",
    "    # Define de the cost function. cp.tv(x) = x_{i+} - x{i}. Results (of interest for the project) are equivalent\n",
    "    obj = cp.norm(((Q@x)[:-1]), 2) + cp.sum_squares(x - v)/(2*lamb)\n",
    "#     obj = cp.tv(x) + cp.sum_squares(x - v)/(2*lamb)\n",
    "\n",
    "    prob = cp.Problem(cp.Minimize(obj))\n",
    "    prob.solve()\n",
    "    \n",
    "    return x.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-texas",
   "metadata": {},
   "source": [
    "### <a name = 'MatQ+Nonneg_Prox'></a> 5.B Minimization of $\\|Qx\\|_2 + \\delta_{\\rm IR_+^N}(x)$\n",
    "\n",
    "In this section we will solve the proximal of $f(x) = \\|Qx\\|_2 + \\delta_{\\rm IR_+^N}(x)$\n",
    "\n",
    "$$\\mathrm{prox}_f(v) = \\arg \\min_{x \\in \\rm IR_+^N}(\\|Qx\\|_1+\\frac{1}{2 \\lambda}||x - v||_2^2)$$\n",
    "\n",
    "Run the next cell to declare `Q_L1_plus_nonneg_prox`. In it, we will hard-code `Q`to be the finite differences matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "automotive-burton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_L1_plus_nonneg_prox_1D(v, lamb):\n",
    "    \n",
    "    if len(v.shape) > 1:\n",
    "        print('WARNING:\\nThis function is designed for 1D signals. Stopping')\n",
    "        return\n",
    "    # Get dimension of v\n",
    "    n = len(v)\n",
    "    \n",
    "    vector = np.zeros((n,))\n",
    "    vector[0] = 1\n",
    "    vector[1] = -1\n",
    "    idx = np.arange(n)\n",
    "    Q = np.matlib.repmat(vector, n, 1)\n",
    "    for i, row in enumerate(Q):\n",
    "        Q[i] = np.roll(row, i)\n",
    "        \n",
    "    # Define variable\n",
    "    x = cp.Variable(n, nonneg = True) \n",
    "    # Define de the cost function. cp.tv(x) = x_{i+} - x{i}. Results (of interest for the project) are equivalent\n",
    "    obj = cp.norm(((Q@x)[:-1]), 2) + cp.sum_squares(x - v)/(2*lamb)\n",
    "#     obj = cp.tv(x) + cp.sum_squares(x - v)/(2*lamb)\n",
    "    prob = cp.Problem(cp.Minimize(obj))\n",
    "    prob.solve()\n",
    "    \n",
    "    return x.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-network",
   "metadata": {},
   "source": [
    "### <a name = 'MatQ_Evaluation'></a> 5.C Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "electoral-agenda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "||Qx||1 Norm, where Q is the finite differences:\n",
      "prox_nonneg(prox_reg(v)) IS NOT equal to prox(reg + nonneg)\n",
      "Max absolute error: 8.177e-02\n",
      "Average absolute error: 2.053e-02\n",
      "\n",
      "prox_reg(prox_nonneg(v)) IS NOT equal to prox(reg + nonneg)\n",
      "Max absolute error: 1.352e-01\n",
      "Average absolute error: 2.965e-02\n",
      "\n",
      "Plotting example\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea81de635b746db98cd242c28a203eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Pan', 'Pan axes with left…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '$\\\\mathrm{prox}_{\\\\lambda (\\\\|Q\\\\cdot \\\\|_1 + \\\\delta)}(\\\\mathbf{v})$')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\n||Qx||1 Norm, where Q is the finite differences:')\n",
    "plt.close('all')\n",
    "proximal.evaluate(Q_L1_prox_1D, Q_L1_plus_nonneg_prox_1D, n=1, size=(30,), mean=0, sigma=1, lamb=0.3, plot = True, rtol=1e-2, atol=1e-3)\n",
    "plt.title('$\\mathrm{prox}_{\\lambda (\\|Q\\cdot \\|_1 + \\delta)}(\\mathbf{v})$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-batman",
   "metadata": {},
   "source": [
    "# <a name = 'L2'></a> 6. L0 Norm\n",
    "[Back to Index](#Index)\n",
    "\n",
    "In this section we will prove the equations provided on [1.A](#Prox), for the $\\mathrm{L}^0$ norm, in a similar way as we did in section [2](#L1). \n",
    "\n",
    "It is defined as:\n",
    "\n",
    "### <a name = 'L2_Prox'></a> 6.A Proximal \n",
    "[Back to Index](#Index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "american-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l0_prox(x, lamb):\n",
    "    x[np.absolute(x) < np.sqrt(2*lamb)] = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "swedish-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l0_plus_nonneg_prox(v, lamb):\n",
    "    \n",
    "    # Get dimension of v\n",
    "    n = v.shape\n",
    "    \n",
    "    # Define variable\n",
    "    x = cp.Variable(n, nonneg = True) \n",
    "    # Define de the cost function\n",
    "    obj = cp.norm((x), 0) + cp.sum_squares(x - v)/(2*lamb)\n",
    "\n",
    "    prob = cp.Problem(cp.Minimize(obj))\n",
    "    prob.solve()\n",
    "    \n",
    "    return x.value\n",
    "\n",
    "def l0_plus_nonneg_prox(x, lamb): \n",
    "    x[x < np.sqrt(2*lamb)] = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-plate",
   "metadata": {},
   "source": [
    "## 7. TV\n",
    "\n",
    "While [section 5](#MatQ) deals with total variation in 1D, in this section we will deal with it in 2D (and just call it Total Variation or $TV$). in [2](http://bigwww.epfl.ch/publications/soubies1904.pdf) it is defined as:\n",
    "\n",
    "$$\\mathcal{R}(Lx) = \\sum^N_{n = 1}\\sqrt{[D_1x]^2_n + [D_2x]^2_n}$$\n",
    "for the Isotropic TV. However, this expression includes a square root, and it is therefore not convex, so it is not easily minimizable. As such we will first try with the Non-isotropic TV:\n",
    "$$\\mathcal{R}(Lx) = \\sum^N_{n = 1}|[D_1x]_n| + |[D_2x]_n|$$\n",
    "\n",
    "Run the next cell to define the functions `TV_prox` and `TV_plus_nonneg_prox`\n",
    "\n",
    "<!-- Q_L1_prox_1D, Q_L1_plus_nonneg_prox_1D -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "opponent-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TV_prox(v, lamb):\n",
    "    \n",
    "    if len(v.shape) != 2:\n",
    "        print('WARNING\\n:This function is  designed for 2D operators. Terminating function.')\n",
    "        return\n",
    "    \n",
    "    nx, ny = v.shape\n",
    "    \n",
    "    x = cp.Variable((nx, ny))\n",
    "    \n",
    "    # Defining D1 and D2x We loose one element in the *other* dimension to account for the lost element and have elements of equal size\n",
    "    D1 = cp.abs((x[:, 1:] - x[:, :-1])[:-1, :])\n",
    "    D2 = cp.abs((x[1:, :] - x[:-1, :])[:, :-1])\n",
    "                \n",
    "    ################### Isometric TV ################    \n",
    "#     obj = cp.pnorm(D1_1 + D2_1, 2) + cp.pnorm(D1_2 +  D2_2, 2) + cp.sum_squares(x - v)/(2*lamb)\n",
    "    \n",
    "    ################### Non-Isometric TV ################\n",
    "    # This objective represents the mixed norm l 1, 1\n",
    "#     obj = cp.pnorm(D1 + D2, 1) + cp.sum_squares(x - v)/(2*lamb)\n",
    "#     obj = cp.pnorm(D1, 1) + cp.pnorm(D2, 1) +cp.sum_squares(x - v)/(2*lamb)\n",
    "    obj = cp.sum(D1) + cp.sum(D2) +cp.sum_squares(x - v)/(2*lamb)\n",
    "    # This objective represents the mixed norm l 2, 1\n",
    "#     obj = cp.sum(cp.sqrt(cp.square(D1) + cp.square(D2))) + cp.sum_squares(x - v)/(2*lamb)\n",
    "    # TV objective says that results might be innaccurate\n",
    "#     obj = cp.tv(x) + cp.sum_squares(x - v)/(2*lamb)\n",
    "    \n",
    "    prob = cp.Problem(cp.Minimize(obj))\n",
    "#     prob.solve(solver=cp.ECOS)\n",
    "    prob.solve()\n",
    "    \n",
    "    return x.value\n",
    "\n",
    "def TV_plus_nonneg_prox(v, lamb):\n",
    "    \n",
    "    if len(v.shape) != 2:\n",
    "        print('WARNING\\n:This function is  designed for 2D operators. Terminating function.')\n",
    "        return\n",
    "    \n",
    "    nx, ny = v.shape\n",
    "    \n",
    "    x = cp.Variable((nx, ny), nonneg = True)\n",
    "    \n",
    "    # Defining D1 and D2x We loose one element in the *other* dimension to account for the lost element and have elements of equal size\n",
    "    D1 = cp.abs((x[:, 1:] - x[:, :-1])[:-1, :])\n",
    "    D2 = cp.abs((x[1:, :] - x[:-1, :])[:, :-1])\n",
    "#     D1 = cp.abs(x[:, 1:] - x[:, :-1])\n",
    "#     D2 = cp.abs(x[1:, :] - x[:-1, :])\n",
    "                \n",
    "    ################### Isometric TV ################    \n",
    "#     obj = cp.pnorm(D1_1 + D2_1, 2) + cp.pnorm(D1_2 +  D2_2, 2) + cp.sum_squares(x - v)/(2*lamb)\n",
    "    \n",
    "    ################### Non-Isometric TV ################\n",
    "    # This objective represents the mixed norm l 1, 1\n",
    "#     obj = cp.pnorm(D1 + D2, 1) + cp.sum_squares(x - v)/(2*lamb)\n",
    "#     obj = cp.pnorm(D1, 1) + cp.pnorm(D2, 1) +cp.sum_squares(x - v)/(2*lamb)\n",
    "    obj = cp.sum(D1) + cp.sum(D2) +cp.sum_squares(x - v)/(2*lamb)\n",
    "    # This objective represents the mixed norm l 2, 1\n",
    "#     obj = cp.sum(cp.sqrt(cp.square(D1) + cp.square(D2))) + cp.sum_squares(x - v)/(2*lamb)\n",
    "    \n",
    "#     obj = cp.tv(x) + cp.sum_squares(x - v)/(2*lamb)\n",
    "    \n",
    "    prob = cp.Problem(cp.Minimize(obj))\n",
    "    prob.solve()\n",
    "    \n",
    "    return x.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-harvard",
   "metadata": {},
   "source": [
    "### 6.B Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "limited-genealogy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TV with 1,1 mixed norm regularizer:\n",
      "prox_nonneg(prox_reg(v)) SEEMS equal to prox(reg + nonneg).\n",
      "Max absolute error: 1.194e-05\n",
      "Average absolute error: 2.122e-07\n",
      "\n",
      "prox_reg(prox_nonneg(v)) SEEMS equal to prox(reg + nonneg)\n",
      "Max absolute error: 4.000e-01\n",
      "Average absolute error: 1.353e-01\n",
      "\n",
      "Plotting example as image.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abfb5268231d47c3a062288189645847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Widgets', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('TV with 1,1 mixed norm regularizer:')\n",
    "proximal.evaluate(TV_prox, TV_plus_nonneg_prox, n=2, size=(12, 12), mean=0, sigma=1, lamb=0.2, plot = True, rtol=1e-3, atol=1e-3, err_thr = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "interesting-angola",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.09803922 0.2        0.29803922 0.4        1.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400, 400)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "# cv.resize(src, dsize = (128, 128), , interpolation = cv.INTER_NEAREST)\n",
    "\n",
    "# print(np.unique(transform.resize(data.shepp_logan_phantom(), (100, 100), order = 0)))\n",
    "print(np.unique(cv.resize(data.shepp_logan_phantom(), dsize = (128, 128),interpolation = cv.INTER_NEAREST)))\n",
    "np.unique(data.shepp_logan_phantom())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "royal-compatibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for prox_(tv + nonneg): -10.0 seconds.\n",
      "Time taken for prox_nonneg(prox_tv): -9.0 seconds.\n",
      "Time taken for prox_tv(prox_nonneg): -11.0 seconds.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e394910e977d425fa7815407c4982cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(layout=Layout(width='80%')), Output(), Output(layout=Layout(width='25%'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Show Widgets', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<iplabs.IPLabViewer at 0x2581be74348>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skimage import color, transform, data\n",
    "import time\n",
    "phantom = transform.resize(color.rgb2gray(data.astronaut()), (128, 128))\n",
    "phantom = cv.resize(data.shepp_logan_phantom(), dsize = (100, 100),interpolation = cv.INTER_NEAREST)\n",
    "# phantom = transform.resize(color.rgb2gray(data.astronaut())[100:300, 100:300], (50, 50))\n",
    "noisy_phantom = phantom + np.random.normal(loc=0, scale=0.08, size=phantom.shape)\n",
    "# rec = transform.iradon(radon_coffee, filter_name = None)\n",
    "# nx, ny = rec.shape \n",
    "# rec = rec[ny//2-200:ny//2+200, nx//2-200:nx//2+200]\n",
    "\n",
    "start = time.time()\n",
    "opt_nonneg_plus_prox =  TV_plus_nonneg_prox(noisy_phantom, 0.05)\n",
    "end = time.time()\n",
    "print(f'Time taken for prox_(tv + nonneg): {np.round(start - end)} seconds.')\n",
    "\n",
    "start = time.time()\n",
    "opt_nonneg_o_prox =  np.maximum(np.zeros_like(noisy_phantom), (TV_prox(noisy_phantom, 0.05)))\n",
    "end = time.time()\n",
    "print(f'Time taken for prox_nonneg(prox_tv): {np.round(start - end)} seconds.')\n",
    "\n",
    "start = time.time()\n",
    "opt_prox_o_nonneg = TV_prox(np.maximum(np.zeros_like(phantom), noisy_phantom), 0.05)\n",
    "end = time.time()\n",
    "print(f'Time taken for prox_tv(prox_nonneg): {np.round(start - end)} seconds.')\n",
    "\n",
    "def snr_db(orig, img):\n",
    "    return 10*np.log10((np.sum(orig**2))/(np.sum((orig-img)**2)))\n",
    "\n",
    "image_list = [phantom, noisy_phantom, opt_nonneg_plus_prox, opt_nonneg_o_prox, opt_prox_o_nonneg, np.abs(opt_nonneg_plus_prox - opt_nonneg_o_prox), np.abs(opt_nonneg_plus_prox - opt_prox_o_nonneg)]\n",
    "title_list = ['Original', f'Noisy (SNR = {np.round(snr_db(phantom, noisy_phantom), 2)} [dB])',\n",
    "              '$\\mathrm{prox}_{\\delta + \\mathcal{R}}$' + f' (SNR = {np.round(snr_db(phantom, opt_nonneg_plus_prox), 2)} [dB])',\n",
    "              '$\\mathrm{prox}_{\\delta}(\\mathrm{prox}_{\\mathcal{R}})$' + f' (SNR = {np.round(snr_db(phantom, opt_nonneg_o_prox), 2)} [dB])',\n",
    "              '$\\mathrm{prox}_{\\mathcal{R}}(\\mathrm{prox}_{\\delta})$' + f' (SNR = {np.round(snr_db(phantom, opt_prox_o_nonneg), 2)} [dB])',\n",
    "              '$\\mathrm{prox}_{\\delta + \\mathcal{R}}$ - $\\mathrm{prox}_{\\delta}(\\mathrm{prox}_{\\mathcal{R}})$',\n",
    "              '$\\mathrm{prox}_{\\delta + \\mathcal{R}}$ - $\\mathrm{prox}_{\\mathcal{R}}(\\mathrm{prox}_{\\delta})$']\n",
    "\n",
    "viewer(image_list, title = title_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-renaissance",
   "metadata": {},
   "source": [
    "## 8 Hessian Schatten Norm\n",
    "\n",
    "$$\\mathcal{R}(Lx) = \\sum^N_{n = 1}\\| \\left[\n",
    "  \\begin{bmatrix}\n",
    "    [D_{11}x]_n [D_{12}x]_n\\\\\n",
    "    [D_{21}x]_n [D_{22}x]_n\n",
    "  \\end{bmatrix}\n",
    "\\right] \\|_*$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hessian_Schatten_prox(v, lamb):\n",
    "    \n",
    "    if len(v.shape) != 2:\n",
    "        print('WARNING\\n:This function is  designed for 2D operators. Terminating function.')\n",
    "        return\n",
    "    \n",
    "    nx, ny = v.shape\n",
    "    \n",
    "    x = cp.Variable((nx, ny))\n",
    "    \n",
    "    # Defining D1 and D2x We loose one element in the *other* dimension to account for the lost element and have elements of equal size\n",
    "    D1 = cp.abs((x[:, 1:] - x[:, :-1])[:-1, :])\n",
    "    D2 = cp.abs((x[1:, :] - x[:-1, :])[:, :-1])\n",
    "    D11 = cp.abs((D1[:, 1:] - D1[:, :-1])[:-1, :])\n",
    "    D22 = cp.abs((D2[1:, :] - D2[:-1, :])[:, :-1])\n",
    "    D21 = cp.abs((D2[:, 1:] - D2[:, :-1])[:-1, :])\n",
    "    D12 = cp.abs((D1[1:, :] - D1[:-1, :])[:, :-1])\n",
    "#     T = D11 + D22\n",
    "#     D = cp.multiply(D11, D22) - cp.multiply(D12, D21)\n",
    "#     lamb_1 = T/2 + cp.multiply((T)**2, cp.power(4 - D, -1))\n",
    "#     lamb_2 = T/2 - cp.multiply((T)**2, cp.power(4 - D, -1))\n",
    "#     y = cp.Variable((nx*ny, 2, 2))\n",
    "#     y[:, 0, 0] = cp.vec(D11)\n",
    "#     y[:, 0, 1] = cp.vec(D12)\n",
    "#     y[:, 1, 0] = cp.vec(D21)\n",
    "#     y[:, 1, 1] = cp.vec(D22)\n",
    "    D11 = cp.vec(D11)\n",
    "    D12 = cp.vec(D12)\n",
    "    D21 = cp.vec(D21)\n",
    "    D22 = cp.vec(D22)\n",
    "    inner = 0\n",
    "    for i in range(nx*ny):\n",
    "#         y = cp.Variable((2, 2))\n",
    "#         y[0, 0] = D11[i]\n",
    "#         y[0, 1] = D12[i]\n",
    "#         y[1, 0] = D21[i]\n",
    "#         y[1, 1] = D22[i]\n",
    "        a = [[D11[i], D12[i]], [D21[i], D22[i]]]\n",
    "        inner = inner + cp.sum_largest(scipy.linalg.eigvals(a), 2) \n",
    "    \n",
    "#     obj = cp.pnorm(lamb_1 + lamb_2, 1) + cp.sum_squares(x - v)/(2*lamb)\n",
    "    obj = inner + cp.sum_squares(x - v)/(2*lamb)\n",
    "    prob = cp.Problem(cp.Minimize(obj))\n",
    "    prob.solve()\n",
    "    \n",
    "    return x.value\n",
    "\n",
    "def Hessian_Schatten_plus_nonneg_prox(v, lamb):\n",
    "    \n",
    "    if len(v.shape) != 2:\n",
    "        print('WARNING\\n:This function is  designed for 2D operators. Terminating function.')\n",
    "        return\n",
    "    \n",
    "    nx, ny = v.shape\n",
    "    \n",
    "    x = cp.Variable((nx, ny), nonneg = True)\n",
    "    \n",
    "    # Defining D1 and D2x We loose one element in the *other* dimension to account for the lost element and have elements of equal size\n",
    "    D1 = (x[:, 1:] - x[:, :-1])[:-1, :]\n",
    "    D2 = (x[1:, :] - x[:-1, :])[:, :-1]\n",
    "    D11 = (D1[:, 1:] - D1[:, :-1])[:-1, :]\n",
    "    D22 = (D2[1:, :] - D2[:-1, :])[:, :-1]\n",
    "    D21 = (D2[:, 1:] - D2[:, :-1])[:-1, :]\n",
    "    D12 = (D1[1:, :] - D1[:-1, :])[:, :-1]\n",
    "#     T = D11 + D22\n",
    "#     D = cp.multiply(D11, D22) - cp.multiply(D12, D21)\n",
    "#     lamb_1 = T/2 + cp.multiply((T)**2, cp.power(4 - D, -1))\n",
    "#     lamb_2 = T/2 - cp.multiply((T)**2, cp.power(4 - D, -1))\n",
    "#     y = cp.Variable((nx*ny, 2, 2))\n",
    "#     y[:, 0, 0] = cp.vec(D11)\n",
    "#     y[:, 0, 1] = cp.vec(D12)\n",
    "#     y[:, 1, 0] = cp.vec(D21)\n",
    "#     y[:, 1, 1] = cp.vec(D22)\n",
    "    D11 = cp.vec(D11)\n",
    "    D12 = cp.vec(D12)\n",
    "    D21 = cp.vec(D21)\n",
    "    D22 = cp.vec(D22)\n",
    "    inner = 0\n",
    "    for i in range(nx*ny):\n",
    "#         y = cp.Variable((2, 2))\n",
    "#         y[0, 0] = D11[i]\n",
    "#         y[0, 1] = D12[i]\n",
    "#         y[1, 0] = D21[i]\n",
    "#         y[1, 1] = D22[i]\n",
    "        a = [[D11[i], D12[i]], [D21[i], D22[i]]]\n",
    "        eig = scipy.linalg.eigvals(a)\n",
    "        inner = inner + eig[0] + eig[1]# cp.sum_largest(scipy.linalg.eigvals(a), 2) \n",
    "    \n",
    "#     obj = cp.pnorm(lamb_1 + lamb_2, 1) + cp.sum_squares(x - v)/(2*lamb)\n",
    "    obj = inner + cp.sum_squares(x - v)/(2*lamb)\n",
    "\n",
    "    \n",
    "    prob = cp.Problem(cp.Minimize(obj))\n",
    "    prob.solve()\n",
    "    \n",
    "    return x.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-edward",
   "metadata": {},
   "source": [
    "# <a name = 'Eval'></a> Evaluation\n",
    "\n",
    "The following function `evaluate` is designed to evaluate a number of random vectors created from normal distributions on the cases presented in [section 1](#Intro):\n",
    "\n",
    "$$\\mathrm{prox}_{L^2 + \\delta_{\\rm I\\!R_+^N }} $$\n",
    "$$ \\mathrm{prox}_{\\mathrm{L}^2}(\\mathrm{prox}_{\\delta_{\\rm I\\!R_+^N}})$$ \n",
    "$$\\mathrm{prox}_{\\delta_{\\rm I\\!R_+^N}}(\\mathrm{prox}_{\\mathcal{R}})$$\n",
    "\n",
    "And searches for equality. It takes as input parameters:\n",
    "* `reg` (function): Closed-form solution of the proximal operator of the regularizer, **Considerar como input problema CVXPY**\n",
    "* `reg_nonneg` (function): Closed-form solution of the proximal operator of the regularizer + $\\delta_{\\rm I\\!R_+^N }$,\n",
    "* `n` (int): Number of vectors\n",
    "* `shape` (tuple): Shape of the vectors\n",
    "* `mean` (scalar): Mean of the vectors\n",
    "* `sigma` (scalar): $\\sigma$ of the normal distribution used to sample the vectors\n",
    "* `lamb` (scalar): Parameter $\\lambda$ to be used in the pointwise operations.\n",
    "\n",
    "It returns:\n",
    "* `reg_nonneg` (boolean): The findings on whether $\\mathrm{prox}_{L^2 + \\delta_{\\rm I\\!R_+^N }} == \\mathrm{prox}_{\\mathrm{L}^2}(\\mathrm{prox}_{\\delta_{\\rm I\\!R_+^N}})$\n",
    "* `nonneg_reg` (boolean): The findings on whether\n",
    "$\\mathrm{prox}_{L^2 + \\delta_{\\rm I\\!R_+^N }} == \\mathrm{prox}_{\\delta_{\\rm I\\!R_+^N}}(\\mathrm{prox}_{\\Re})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-sociology",
   "metadata": {},
   "source": [
    "Now we will run the function for the L1 and L2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('L2 Norm:')\n",
    "# proximal.evaluate(l2_prox, l2_plus_nonneg_prox, n=100, size=(100,), mean=0, sigma=1, lamb=3, plot=True,)\n",
    "# print('\\nL1 Norm:')\n",
    "# proximal.evaluate(l1_prox, l1_plus_nonneg_prox, n=100, size=(100,), mean=0, sigma=1, lamb=1, plot=True )\n",
    "# print('\\n||Qx||1 Norm, where Q is the DCT:')\n",
    "# proximal.evaluate(DCT_l1_prox, DCT_l1_plus_nonneg, n=100, size=(100,), mean=0, sigma=1, lamb=0.1)\n",
    "# print('\\n||Qx||1 Norm, where Q is the finite differences:')\n",
    "# proximal.evaluate(Q_L1_prox_1D, Q_L1_plus_nonneg_prox_1D, n=100, size=(50,), mean=0, sigma=1, lamb=0.5, plot = True,\n",
    "#                   rtol=1e-2, atol=1e-3)\n",
    "# print('\\n$||x||_0$ Norm')\n",
    "# proximal.evaluate(l0_prox, l0_plus_nonneg_prox, n=10, size=(50,), mean=0, sigma=1, lamb=1, plot = True,\n",
    "#                   rtol=1e-2, atol=1e-3)\n",
    "print('\\nTV 2,1 Regularizer:')\n",
    "proximal.evaluate(TV_prox, TV_plus_nonneg_prox, n=2, size=(20, 20), mean=0, sigma=1, lamb=0.2, plot = True, rtol=1e-3, atol=1e-3)\n",
    "# print('\\nTV(x) Regularizer:')\n",
    "# proximal.evaluate(Hessian_Schatten_prox, Hessian_Schatten_plus_nonneg_prox, n=10, size=(20, 20), mean=0, sigma=1, lamb=0.2, plot = True, rtol=1e-3, atol=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-blake",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1.](https://arc.aiaa.org/doi/pdf/10.2514/1.26320?casa_token=EpyiTfodqo4AAAAA%3AGmnFHyrImbNxMMk2ONs1c9wpN6bLTip8_a7irQweswoms0vMBtL1kGu6h8v6IK76_zhhhtk0KPA&) Pol del Aguila Pla and Joakim Jaldén, Cell detection by functional inverse diffusion and non-negative group sparsity—Part II: Proximal optimization and Performance evaluation, IEEE Transactions on Signal Processing, vol. 66, no. 20, pp. 5422–5437, 2018\n",
    "\n",
    "[2.](http://bigwww.epfl.ch/publications/soubies1904.pdf) E. Soubies, F. Soulez, M.T. McCann, T.-a. Pham, L. Donati, T. Debarre, D. Sage, M. Unser, \"Pocket Guide to Solve Inverse Problems with GlobalBioIm,\" Inverse Problems, vol. 35, no. 10, paper no. 104006, pp. 1-20, October 2019"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
